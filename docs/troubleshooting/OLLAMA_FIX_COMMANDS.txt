# Ollama Fix Commands for Jetson - "No Output from LLM" Issue
# Run these commands one at a time on your Jetson device

# 1. Stop Ollama service
sudo systemctl stop ollama

# 2. Wait a moment, then check it's stopped
sudo systemctl status ollama

# 3. Clear potentially corrupted model cache
sudo rm -rf /usr/share/ollama/.ollama/models/manifests/*

# 4. Clear corrupted model blobs
sudo rm -rf /usr/share/ollama/.ollama/models/blobs/sha256-*

# 5. Start Ollama service
sudo systemctl start ollama

# 6. Wait 10 seconds, then check service status
sleep 10
sudo systemctl status ollama

# 7. Test Ollama API connectivity
curl http://localhost:11434/api/version

# 8. Pull LIGHTER model for Jetson (saves memory - only ~1GB)
ollama pull llama3.2:1b

# 9. SKIP vision model for now to save memory
# ollama pull llava-llama3

# 10. Test simple generation with LIGHTER model
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{"model": "llama3.2:1b", "prompt": "Say hello", "stream": false}'

# 11. Test LIGHTWEIGHT Lex configuration 
uv run src/run.py lex_channel_chief_jetson_lite

# ===== TROUBLESHOOTING COMMANDS (if problems persist) =====

# Check logs in real-time (Ctrl+C to exit)
# sudo journalctl -u ollama -f

# Check available models
# ollama list

# Run diagnostic script
# python testing/diagnose_ollama_jetson.py

# Check system resources
# free -h
# df -h /usr/share/ollama

# Restart Ollama if needed
# sudo systemctl restart ollama

# ===== CAMERA FIX COMMANDS (if camera not working) =====

# Check camera permissions
# ls -la /dev/video*

# Fix camera permissions
# sudo chmod 666 /dev/video0

# Check available cameras
# v4l2-ctl --list-devices

# Test camera directly
# ffmpeg -f v4l2 -i /dev/video0 -frames:v 1 test_camera.jpg

# ===== PERSISTENT OLLAMA ISSUES (if error returns) =====

# Check if Ollama is actually responding
# curl -v http://localhost:11434/api/version

# Monitor Ollama memory usage
# sudo ps aux | grep ollama

# Check Ollama configuration
# sudo systemctl cat ollama

# Set memory limits for Jetson
# sudo systemctl edit ollama
# Add these lines:
# [Service]
# Environment=OLLAMA_MAX_LOADED_MODELS=1
# Environment=OLLAMA_NUM_PARALLEL=1
# Environment=OLLAMA_MAX_QUEUE=1

# Restart with new config
# sudo systemctl daemon-reload
# sudo systemctl restart ollama

# Test with timeout using LIGHTER model
# timeout 30 curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d '{"model": "llama3.2:1b", "prompt": "test", "stream": false}'

# ===== NOTES =====
# - Steps 8-9 download ~10GB total, ensure good internet
# - If curl commands show JSON responses, that's good
# - If you see "Load failed" in logs, the cache clearing should fix it
# - The HTTP 500/499 errors you saw indicate corrupted model cache
# - Camera now working at index 1, vision should function
# - If Ollama keeps failing, may need memory/queue limits for Jetson